{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import requests\n",
    "from tqdm.autonotebook import tqdm\n",
    "import json\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For purposes of this code please go to the: https://data.police.uk/data/archive/ and upload following datasets:\n",
    "(or any other combination of datasets such that you would include all months from 2016-2023)\n",
    "- December 2018\n",
    "- December 2021\n",
    "- April 2024\n",
    "\n",
    "Make them regular folders (meaning if they are zip extract them from zip, get the folder to the downloads directory and you can delete the zip)\n",
    "after this file is done running and you got all the needed files you can delete the folders with dates which you uploaded since they will be empty\n",
    "\n",
    "Also download the PAS_borough file from https://data.london.gov.uk/dataset/mopac-surveys and after running the code just put it in the folder where you have the rest of your data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading the data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- in dates provide names of the files if they differ main point is to get as many files as possible with lowest amount of downloads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your Path variable to the new folder where project would be saved\n",
    "PATH = '/Users/ansat.omurzakov/Desktop/DBLchik/' # give a path where you would love to store the newly uploaded files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# names of folders\n",
    "dates = ['2018-12', '2021-12', '2024-04']\n",
    "\n",
    "# provide path where all of the folders lie (please place them on the same level)\n",
    "original_folders = [f'/Users/ansat.omurzakov/Downloads/{date}' for date in dates]\n",
    "\n",
    "# Create the new folder if it doesn't exist\n",
    "os.makedirs(PATH, exist_ok=True)\n",
    "\n",
    "# Function to generate a unique destination path\n",
    "def get_unique_path(dst_folder, name):\n",
    "    counter = 1\n",
    "    base, extension = os.path.splitext(name)\n",
    "    unique_name = name\n",
    "    unique_path = os.path.join(dst_folder, unique_name)\n",
    "\n",
    "    while os.path.exists(unique_path):\n",
    "        unique_name = f\"{base}_{counter}{extension}\"\n",
    "        unique_path = os.path.join(dst_folder, unique_name)\n",
    "        counter += 1\n",
    "\n",
    "    return unique_path\n",
    "\n",
    "# Loop through each of the original folders\n",
    "for folder in original_folders:\n",
    "    # List all items in the current folder\n",
    "    items = os.listdir(folder)\n",
    "    \n",
    "    for item in items:\n",
    "        # Construct full item path\n",
    "        item_path = os.path.join(folder, item)\n",
    "        \n",
    "        # Get a unique destination path\n",
    "        destination_path = get_unique_path(PATH, item)\n",
    "        \n",
    "        # Move each item to the new folder\n",
    "        shutil.move(item_path, destination_path)\n",
    "\n",
    "print(f\"All contents moved to {PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting copies of files\n",
    "for file in os.listdir(PATH):\n",
    "    if re.search(r'\\d{4}-\\d{2}_1', file):\n",
    "        shutil.rmtree(PATH +  file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_files(PATH):\n",
    "    # getting dates from the CSV file\n",
    "    dates_list = os.listdir(PATH)\n",
    "    for date in dates_list:\n",
    "        if not re.match(r'\\d{4}-\\d{2}', date):\n",
    "            dates_list.remove(date)\n",
    "\n",
    "    dates_list = sorted(dates_list)\n",
    "\n",
    "    # deleting irrelevant files from directories\n",
    "    for date in tqdm(dates_list, total = len(dates_list)):\n",
    "        for file in os.listdir(PATH + date):\n",
    "            if not re.search(r'(metropolitan-street)|(metropolitan-stop-and-search)', file):\n",
    "                os.remove(PATH + date + '/' + file)\n",
    "    return 'Done!'\n",
    "\n",
    "def save_data(category, PATH):\n",
    "    if len(category) != 0:\n",
    "        all_data = pd.DataFrame()\n",
    "        for f in tqdm(category, total = len(category)):\n",
    "            df = pd.read_csv(PATH + f[:7] + '/' + f)\n",
    "            all_data = pd.concat([all_data,df],ignore_index=True)\n",
    "            all_data = all_data.drop_duplicates(keep = 'first')\n",
    "        all_data.to_csv(PATH + re.split(r'(\\d{2}-)', category[0])[-1], index=False)\n",
    "    else: return f'List {category} is empty'\n",
    "\n",
    "def get_file_list(department, type, PATH):\n",
    "    lst = []\n",
    "    for _, _, files in os.walk(PATH, topdown = True):\n",
    "        for filename in files:\n",
    "            if f'{department}-{type}' in filename:\n",
    "                lst.append(filename)\n",
    "    return lst"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deleting irrelevant files from directories (neighborhood, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_files(PATH)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting file names from which to retrieve csvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metropolitan_files\n",
    "metropolitan_street = get_file_list('metropolitan','street', PATH)\n",
    "metropolitan_sas= get_file_list('metropolitan','stop-and-search', PATH)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenating files per category and saving them into one csv file (takes quite some portion of time due to the fact that we are uploading a lot of files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metropolitan Data\n",
    "save_data(metropolitan_street, PATH)\n",
    "print('[INFO] Combining datasets to Metropolitan-street.csv is done')\n",
    "save_data(metropolitan_sas, PATH)\n",
    "print('[INFO] Combining datasets to Metropolitan-stop-and-search.csv is done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove non-concatenated files\n",
    "for file in os.listdir(PATH):\n",
    "    if re.search(r'\\d{4}-\\d{2}', file):\n",
    "        shutil.rmtree(PATH +  file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting all neighbourhoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locate_neighbourhood_link = 'https://data.police.uk/api/metropolitan/neighbourhoods'\n",
    "met_neighbourhoods = requests.get(locate_neighbourhood_link).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbourhoods = [neighbourhood['id'] for neighbourhood in met_neighbourhoods]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify neighbourhoods to boroughs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = {}\n",
    "for value in tqdm(neighbourhoods, total = len(neighbourhoods)):\n",
    "    try:\n",
    "        data = requests.get(f'https://findthatpostcode.uk/areas/{value}.json').json()['included']\n",
    "        name = data[6]['attributes']['name']\n",
    "        if name in dic.keys():\n",
    "            dic[name].append(value)\n",
    "        else:\n",
    "            dic[name] = [value]\n",
    "    except Exception as e:\n",
    "        data = requests.get(f'https://findthatpostcode.uk/areas/{value[:-1]}.json').json()['included']\n",
    "        name = data[6]['attributes']['name']\n",
    "        if name in dic.keys():\n",
    "            dic[name].append(value)\n",
    "        else:\n",
    "            dic[name] = [value]\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(PATH + 'boroughs_neighbourhoods.json', 'w') as file:\n",
    "    json.dump(dic, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load JSON data from file\n",
    "with open(PATH + 'boroughs_neighbourhoods.json', 'r') as file:\n",
    "    json_data = json.load(file)\n",
    "\n",
    "# Convert JSON data to a DataFrame\n",
    "boroughs_data = []\n",
    "for borough, area_codes in json_data.items():\n",
    "    for area_code in area_codes:\n",
    "        boroughs_data.append({'Borough': borough, 'Area Code': area_code})\n",
    "\n",
    "boroughs_neighbours = pd.DataFrame(boroughs_data)\n",
    "boroughs_neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_n_b = {}\n",
    "\n",
    "for borough in tqdm(boroughs_neighbours['Borough'].unique(), total=len(boroughs_neighbours['Borough'].unique())):\n",
    "    if borough not in b_n_b:\n",
    "        b_n_b[borough] = []\n",
    "    for neighbourhood in boroughs_neighbours[boroughs_neighbours['Borough'] == borough]['Area Code'].unique():\n",
    "        boundary_points = [(item['latitude'], item['longitude']) for item in requests.get(f'https://data.police.uk/api/metropolitan/{neighbourhood}/boundary').json()]\n",
    "        b_n_b[borough].append({neighbourhood: boundary_points})\n",
    "    print(f'[INFO] Added neighbourhoods and their boundaries for {borough}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the file path\n",
    "json_file_path = PATH + \"/neighborhood_boundaries.json\"\n",
    "\n",
    "# Save the dictionary as a JSON file\n",
    "with open(json_file_path, \"w\") as json_file:\n",
    "    json.dump(b_n_b, json_file)\n",
    "\n",
    "print(\"JSON file saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_variable = Path(PATH)\n",
    "\n",
    "# Save the variable to a file\n",
    "with open('path_variable.pkl', 'wb') as f:\n",
    "    pickle.dump(path_variable, f)\n",
    "    print('PATH variable saved successfully')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
